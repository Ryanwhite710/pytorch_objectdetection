{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dcc587e",
   "metadata": {},
   "source": [
    "Configuration:\n",
    "PyTorch Version: 1.12.1+cu116\n",
    "Python: 3.9\n",
    "CUDA: 11.6\n",
    "Virtual Environment: .venv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1151ead",
   "metadata": {},
   "source": [
    "Install necessary packages in your environment:\n",
    "pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db8b779a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\X039784\\projects\\pytorch_objectdetection\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74999ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Paths\n",
    "ROOT_DIR = \"C:/Users/X039784/projects/pytorch_objectdetection\"\n",
    "DATASET_DIR = os.path.join(ROOT_DIR, \"dataset\")\n",
    "TRAIN_IMAGES = os.path.join(DATASET_DIR, \"train\")\n",
    "VAL_IMAGES = os.path.join(DATASET_DIR, \"valid\")\n",
    "TEST_IMAGES = os.path.join(DATASET_DIR, \"test\")\n",
    "TRAIN_ANNOTATIONS = os.path.join(DATASET_DIR, \"train/_annotations.coco.json\")\n",
    "VAL_ANNOTATIONS = os.path.join(DATASET_DIR, \"valid/_annotations.coco.json\")\n",
    "TEST_ANNOTATIONS = os.path.join(DATASET_DIR, \"test/_annotations.coco.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89dfa463",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd694f6b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Custom collate function to handle multi-class and multi-bounding box scenarios\n",
    "def collate_fn_safe(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    processed_targets = []\n",
    "    for target in targets:\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for obj in target:\n",
    "            # Convert [x, y, width, height] to [x_min, y_min, x_max, y_max]\n",
    "            x_min, y_min, width, height = obj[\"bbox\"]\n",
    "            x_max = x_min + width\n",
    "            y_max = y_min + height\n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "            labels.append(obj[\"category_id\"])\n",
    "        processed_targets.append({\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64)\n",
    "        })\n",
    "    return list(images), processed_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9134804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Dataset and DataLoader\n",
    "train_dataset = CocoDetection(\n",
    "    root=TRAIN_IMAGES,\n",
    "    annFile=TRAIN_ANNOTATIONS,\n",
    "    transform=train_transforms\n",
    ")\n",
    "\n",
    "val_dataset = CocoDetection(\n",
    "    root=VAL_IMAGES,\n",
    "    annFile=VAL_ANNOTATIONS,\n",
    "    transform=val_transforms\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0, collate_fn=collate_fn_safe)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0, collate_fn=collate_fn_safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4ee6dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\X039784\\projects\\pytorch_objectdetection\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\X039784\\projects\\pytorch_objectdetection\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load Faster R-CNN and modify for 3 classes\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "num_classes = 4  # Background + 3 classes\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e3082e8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae5f0297",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Training and Validation Functions\n",
    "def train_one_epoch(model, optimizer, data_loader, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, targets in data_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Forward pass and loss calculation\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += losses.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def validate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            outputs = model(images)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69975ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/10: 100%|██████████| 38/38 [00:42<00:00,  1.13s/it, Batch Loss=0.402]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.3339\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/10: 100%|██████████| 38/38 [00:39<00:00,  1.04s/it, Batch Loss=0.0568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.1852\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/10: 100%|██████████| 38/38 [00:38<00:00,  1.01s/it, Batch Loss=0.0542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.1410\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/10: 100%|██████████| 38/38 [00:37<00:00,  1.01it/s, Batch Loss=0.0232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.1075\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/10: 100%|██████████| 38/38 [00:37<00:00,  1.02it/s, Batch Loss=0.121] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 0.0916\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6/10: 100%|██████████| 38/38 [00:39<00:00,  1.03s/it, Batch Loss=0.0444]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 0.0770\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7/10: 100%|██████████| 38/38 [00:39<00:00,  1.04s/it, Batch Loss=0.14]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 0.0700\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8/10: 100%|██████████| 38/38 [00:39<00:00,  1.04s/it, Batch Loss=0.0494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: 0.0631\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9/10: 100%|██████████| 38/38 [00:39<00:00,  1.03s/it, Batch Loss=0.0224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss: 0.0600\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10/10: 100%|██████████| 38/38 [00:38<00:00,  1.03s/it, Batch Loss=0.0331]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss: 0.0516\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    total_loss = 0\n",
    "    for images, targets in progress_bar:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Forward pass and loss calculation\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update loss and progress bar\n",
    "        total_loss += losses.item()\n",
    "        progress_bar.set_postfix({\"Batch Loss\": losses.item()})\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), os.path.join(ROOT_DIR, \"faster_rcnn_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b77a1fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "[{'boxes': tensor([[  0.0000,  27.9284, 640.0000, 640.0000]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([0.9965], device='cuda:0', grad_fn=<IndexBackward0>)}]\n",
      "[{'boxes': tensor([[  3.7759,  10.6436, 640.0000, 640.0000]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([0.9914], device='cuda:0', grad_fn=<IndexBackward0>)}]\n",
      "[{'boxes': tensor([[8.2945e+00, 4.1831e+01, 6.4000e+02, 6.4000e+02],\n",
      "        [6.3995e+02, 8.1426e-02, 6.4000e+02, 1.0339e+01],\n",
      "        [6.3992e+02, 1.7961e+00, 6.4000e+02, 5.2874e+01],\n",
      "        [6.3995e+02, 8.3080e-03, 6.4000e+02, 1.0443e+01],\n",
      "        [6.3992e+02, 1.4320e+00, 6.4000e+02, 5.3393e+01],\n",
      "        [6.3995e+02, 5.5721e-02, 6.4000e+02, 1.0299e+01],\n",
      "        [6.3992e+02, 1.6681e+00, 6.4000e+02, 5.2675e+01]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>), 'labels': tensor([1, 2, 2, 1, 1, 3, 3], device='cuda:0'), 'scores': tensor([0.9967, 0.1633, 0.1633, 0.1556, 0.1556, 0.1473, 0.1473],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)}]\n",
      "[{'boxes': tensor([[ 17.6935,  11.6248, 637.6250, 640.0000],\n",
      "        [ 36.0862,   0.0000, 640.0000, 640.0000]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>), 'labels': tensor([1, 3], device='cuda:0'), 'scores': tensor([0.9751, 0.1839], device='cuda:0', grad_fn=<IndexBackward0>)}]\n",
      "[{'boxes': tensor([[  2.1637,  10.0529, 640.0000, 640.0000]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([0.9981], device='cuda:0', grad_fn=<IndexBackward0>)}]\n",
      "[{'boxes': tensor([[ 15.5144,  14.5953, 640.0000, 640.0000]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([0.9964], device='cuda:0', grad_fn=<IndexBackward0>)}]\n",
      "[{'boxes': tensor([[  9.9690,  14.4672, 640.0000, 635.1227],\n",
      "        [639.9779, 272.4302, 640.0000, 284.9698],\n",
      "        [639.9780, 272.3408, 640.0000, 285.0972],\n",
      "        [639.9778, 272.3988, 640.0000, 284.9211]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>), 'labels': tensor([1, 2, 1, 3], device='cuda:0'), 'scores': tensor([0.9953, 0.1633, 0.1556, 0.1473], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)}]\n",
      "[{'boxes': tensor([[  1.8894,  17.2215, 640.0000, 640.0000]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([0.9984], device='cuda:0', grad_fn=<IndexBackward0>)}]\n",
      "[{'boxes': tensor([[ 19.4946,  18.8076, 640.0000, 640.0000],\n",
      "        [639.9450,   0.0000, 640.0000,   6.5449],\n",
      "        [639.9451,   0.0000, 640.0000,   6.6118],\n",
      "        [639.9449,   0.0000, 640.0000,   6.5193],\n",
      "        [ 21.1666,   0.0000, 640.0000, 623.7540]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>), 'labels': tensor([1, 2, 1, 3, 3], device='cuda:0'), 'scores': tensor([0.9821, 0.1633, 0.1556, 0.1473, 0.0862], device='cuda:0',\n",
      "       grad_fn=<IndexBackward0>)}]\n",
      "[{'boxes': tensor([[  1.0527,  14.2103, 640.0000, 640.0000],\n",
      "        [639.9135,   3.0425, 640.0000,  15.0104],\n",
      "        [639.9074,  13.7143, 640.0000,  26.2069],\n",
      "        [639.9136,   2.9572, 640.0000,  15.1320],\n",
      "        [639.9075,  13.6253, 640.0000,  26.3339],\n",
      "        [639.9133,   3.0126, 640.0000,  14.9639],\n",
      "        [639.9071,  13.6830, 640.0000,  26.1584],\n",
      "        [ 27.0632,   0.0000, 640.0000, 605.0783]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>), 'labels': tensor([1, 2, 2, 1, 1, 3, 3, 3], device='cuda:0'), 'scores': tensor([0.9947, 0.1633, 0.1633, 0.1556, 0.1556, 0.1473, 0.1473, 0.0729],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)}]\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "test_dataset = CocoDetection(root=TEST_IMAGES, annFile=TEST_ANNOTATIONS, transform=val_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn_safe)\n",
    "\n",
    "model.eval()\n",
    "for images, targets in test_loader:\n",
    "    images = list(image.to(device) for image in images)\n",
    "    outputs = model(images)\n",
    "    print(outputs)  # Outputs contain predicted boxes, labels, and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70b3b5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\X039784\\projects\\pytorch_objectdetection\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model_path = os.path.join(ROOT_DIR, \"faster_rcnn_model.pth\")\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=False)  # Initialize the model\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)  # Match architecture\n",
    "model.load_state_dict(torch.load(model_path))  # Load trained weights\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f8a01da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class InferenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_folder, transforms=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.transforms = transforms\n",
    "        self.image_files = [f for f in os.listdir(image_folder) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_folder, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        return image, self.image_files[idx]  # Return the image and its file name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4619c3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dataset = InferenceDataset(\n",
    "    image_folder=TEST_IMAGES,\n",
    "    transforms=val_transforms  # Use the same transforms as validation\n",
    ")\n",
    "\n",
    "inference_loader = DataLoader(inference_dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc261196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: bulk-pallets-package-boxes-loaded-260nw-2532443573_webp.rf.92995f9dd075ddb19781bdf8025d778a.jpg\n",
      "  Detected: Material\n",
      "  Confidence: 0.99\n",
      "  Box: [81.1865005493164, 130.14952087402344, 620.90771484375, 545.271728515625]\n",
      "Image: Column-Lift-Gate.jpg\n",
      "  Detected: Material\n",
      "  Confidence: 0.89\n",
      "  Box: [337.2694396972656, 43.20225524902344, 549.3857421875, 332.69500732421875]\n",
      "  Detected: Not Empty Trailer\n",
      "  Confidence: 0.79\n",
      "  Box: [25.339126586914062, 0.0, 468.54754638671875, 362.2803955078125]\n",
      "  Detected: Not Empty Trailer\n",
      "  Confidence: 0.70\n",
      "  Box: [227.46583557128906, 3.2146759033203125, 584.035400390625, 368.34033203125]\n",
      "Image: download-14-_jpg.rf.d1ef41ec770808b2374febcd98c449b8.jpg\n",
      "  Detected: Empty Trailer\n",
      "  Confidence: 1.00\n",
      "  Box: [2.963574171066284, 16.687744140625, 640.0, 640.0]\n",
      "Image: download-22-_jpg.rf.7b88bce0eeee9c5733edec70615bfc9c.jpg\n",
      "  Detected: Empty Trailer\n",
      "  Confidence: 1.00\n",
      "  Box: [10.0782470703125, 13.991601943969727, 640.0, 635.81982421875]\n",
      "Image: download-23-_jpg.rf.e9f3eb1de7ff56499f9c2c5277f2857b.jpg\n",
      "  Detected: Empty Trailer\n",
      "  Confidence: 1.00\n",
      "  Box: [16.267309188842773, 14.2039794921875, 640.0, 640.0]\n",
      "Image: download-29-_jpg.rf.d7f6f487bfc811507d08741756229f37.jpg\n",
      "  Detected: Empty Trailer\n",
      "  Confidence: 1.00\n",
      "  Box: [6.8638916015625, 36.64699935913086, 640.0, 640.0]\n",
      "Image: download-3-_jpg.rf.58f8278f9d7a5406e2f37e9086ffed2f.jpg\n",
      "  Detected: Empty Trailer\n",
      "  Confidence: 0.99\n",
      "  Box: [4.092797756195068, 5.613037109375, 640.0, 640.0]\n",
      "Image: download-6-_jpg.rf.7f201cb5a42204bff1c223df0d113cd6.jpg\n",
      "  Detected: Empty Trailer\n",
      "  Confidence: 0.97\n",
      "  Box: [18.583642959594727, 10.509033203125, 637.7030639648438, 640.0]\n",
      "Image: Dry-Van-Trailers-UTILITY-BLACK-OWNER-OPERATOR-SPEC-DRY-VAN-45021395.jpg\n",
      "  Detected: Empty Trailer\n",
      "  Confidence: 0.97\n",
      "  Box: [108.47864532470703, 28.71784019470215, 1157.9088134765625, 1157.0]\n",
      "  Detected: Material\n",
      "  Confidence: 0.59\n",
      "  Box: [88.697021484375, 674.3892211914062, 204.5599822998047, 769.5496215820312]\n",
      "Image: how-many-pallets-fit-in-a-container.jpg\n",
      "  Detected: Material\n",
      "  Confidence: 0.99\n",
      "  Box: [139.0110626220703, 145.5214080810547, 242.07754516601562, 220.13177490234375]\n",
      "  Detected: Material\n",
      "  Confidence: 0.94\n",
      "  Box: [327.1756591796875, 153.42495727539062, 375.0, 240.87405395507812]\n",
      "  Detected: Material\n",
      "  Confidence: 0.93\n",
      "  Box: [127.64619445800781, 56.11046600341797, 252.62709045410156, 175.08670043945312]\n",
      "  Detected: Not Empty Trailer\n",
      "  Confidence: 0.89\n",
      "  Box: [89.8475570678711, 0.04502296447753906, 375.0, 242.7043914794922]\n",
      "  Detected: Not Empty Trailer\n",
      "  Confidence: 0.80\n",
      "  Box: [1.4494132995605469, 0.0, 191.49307250976562, 231.5928497314453]\n",
      "  Detected: Material\n",
      "  Confidence: 0.79\n",
      "  Box: [11.949224472045898, 27.31130599975586, 124.87210083007812, 207.0465087890625]\n",
      "Image: images (25).jpg\n",
      "  Detected: Empty Trailer\n",
      "  Confidence: 0.81\n",
      "  Box: [6.3625922203063965, 12.168532371520996, 164.28091430664062, 194.0]\n",
      "  Detected: Not Empty Trailer\n",
      "  Confidence: 0.75\n",
      "  Box: [40.491455078125, 0.0, 259.0, 194.0]\n",
      "  Detected: Material\n",
      "  Confidence: 0.71\n",
      "  Box: [110.84596252441406, 16.057525634765625, 213.43858337402344, 138.59019470214844]\n",
      "Image: images (26).jpg\n",
      "  Detected: Empty Trailer\n",
      "  Confidence: 0.83\n",
      "  Box: [24.47631072998047, 9.540345191955566, 263.8839111328125, 168.0]\n",
      "  Detected: Material\n",
      "  Confidence: 0.69\n",
      "  Box: [227.42469787597656, 48.61362075805664, 300.0, 148.01950073242188]\n",
      "  Detected: Not Empty Trailer\n",
      "  Confidence: 0.65\n",
      "  Box: [202.80740356445312, 0.0, 299.4912414550781, 168.0]\n",
      "Image: images-12-_jpg.rf.af21b65142f60592f4b83de730496fc7.jpg\n",
      "  Detected: Empty Trailer\n",
      "  Confidence: 0.99\n",
      "  Box: [0.02680664137005806, 15.2806396484375, 640.0, 640.0]\n",
      "Image: images-14-_jpg.rf.65d2e5362d768e6d61473b6a8dc5fe0f.jpg\n",
      "  Detected: Empty Trailer\n",
      "  Confidence: 1.00\n",
      "  Box: [1.558862328529358, 9.678613662719727, 640.0, 640.0]\n",
      "Image: images-6-_jpg.rf.4c120fe318928b014756341a9bcd967b.jpg\n",
      "  Detected: Empty Trailer\n",
      "  Confidence: 0.98\n",
      "  Box: [20.8956298828125, 18.64892578125, 640.0, 640.0]\n",
      "Image: images-8-_jpg.rf.6dcf5795c1ede532513234f97ff42b0a.jpg\n",
      "  Detected: Empty Trailer\n",
      "  Confidence: 1.00\n",
      "  Box: [0.0, 27.98127555847168, 640.0, 640.0]\n",
      "Image: images.jpg\n",
      "  Detected: Empty Trailer\n",
      "  Confidence: 0.73\n",
      "  Box: [7.445995330810547, 8.705611228942871, 210.15509033203125, 175.97715759277344]\n",
      "  Detected: Empty Trailer\n",
      "  Confidence: 0.55\n",
      "  Box: [122.51494598388672, 2.7637126445770264, 257.18707275390625, 194.0]\n"
     ]
    }
   ],
   "source": [
    "from torchvision.ops import nms\n",
    "\n",
    "# Define a function to process and print detections\n",
    "def process_detections(output, image_name, class_labels, confidence_threshold=0.5, nms_threshold=0.5):\n",
    "    # Extract detection data\n",
    "    boxes = output['boxes'].detach().cpu()\n",
    "    scores = output['scores'].detach().cpu()\n",
    "    labels = output['labels'].detach().cpu()\n",
    "\n",
    "    # Apply confidence threshold\n",
    "    valid_indices = scores > confidence_threshold\n",
    "    boxes = boxes[valid_indices]\n",
    "    scores = scores[valid_indices]\n",
    "    labels = labels[valid_indices]\n",
    "\n",
    "    # Apply Non-Max Suppression (NMS)\n",
    "    if len(boxes) > 0:\n",
    "        keep_indices = nms(boxes, scores, nms_threshold)\n",
    "        boxes = boxes[keep_indices]\n",
    "        scores = scores[keep_indices]\n",
    "        labels = labels[keep_indices]\n",
    "\n",
    "    # Print detections\n",
    "    print(f\"Image: {image_name}\")\n",
    "    for i in range(len(boxes)):\n",
    "        print(f\"  Detected: {class_labels[labels[i].item()]}\")\n",
    "        print(f\"  Confidence: {scores[i].item():.2f}\")\n",
    "        print(f\"  Box: {boxes[i].tolist()}\")\n",
    "\n",
    "\n",
    "# Class labels\n",
    "class_labels = [\"Background\", \"Empty Trailer\", \"Material\", \"Not Empty Trailer\"]  # Replace with your class names\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    for images, image_names in inference_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        outputs = model(images)  # Perform inference\n",
    "        for output, image_name in zip(outputs, image_names):\n",
    "            process_detections(output, image_name, class_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "033f5954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualization to C:/Users/X039784/projects/pytorch_objectdetection\\visualized_results\\detected_bulk-pallets-package-boxes-loaded-260nw-2532443573_webp.rf.92995f9dd075ddb19781bdf8025d778a.jpg\n",
      "Saved visualization to C:/Users/X039784/projects/pytorch_objectdetection\\visualized_results\\detected_Column-Lift-Gate.jpg\n",
      "Saved visualization to C:/Users/X039784/projects/pytorch_objectdetection\\visualized_results\\detected_download-14-_jpg.rf.d1ef41ec770808b2374febcd98c449b8.jpg\n",
      "Saved visualization to C:/Users/X039784/projects/pytorch_objectdetection\\visualized_results\\detected_download-22-_jpg.rf.7b88bce0eeee9c5733edec70615bfc9c.jpg\n",
      "Saved visualization to C:/Users/X039784/projects/pytorch_objectdetection\\visualized_results\\detected_download-23-_jpg.rf.e9f3eb1de7ff56499f9c2c5277f2857b.jpg\n",
      "Saved visualization to C:/Users/X039784/projects/pytorch_objectdetection\\visualized_results\\detected_download-29-_jpg.rf.d7f6f487bfc811507d08741756229f37.jpg\n",
      "Saved visualization to C:/Users/X039784/projects/pytorch_objectdetection\\visualized_results\\detected_download-3-_jpg.rf.58f8278f9d7a5406e2f37e9086ffed2f.jpg\n",
      "Saved visualization to C:/Users/X039784/projects/pytorch_objectdetection\\visualized_results\\detected_download-6-_jpg.rf.7f201cb5a42204bff1c223df0d113cd6.jpg\n",
      "Saved visualization to C:/Users/X039784/projects/pytorch_objectdetection\\visualized_results\\detected_Dry-Van-Trailers-UTILITY-BLACK-OWNER-OPERATOR-SPEC-DRY-VAN-45021395.jpg\n",
      "Saved visualization to C:/Users/X039784/projects/pytorch_objectdetection\\visualized_results\\detected_how-many-pallets-fit-in-a-container.jpg\n",
      "Saved visualization to C:/Users/X039784/projects/pytorch_objectdetection\\visualized_results\\detected_images (25).jpg\n",
      "Saved visualization to C:/Users/X039784/projects/pytorch_objectdetection\\visualized_results\\detected_images (26).jpg\n",
      "Saved visualization to C:/Users/X039784/projects/pytorch_objectdetection\\visualized_results\\detected_images-12-_jpg.rf.af21b65142f60592f4b83de730496fc7.jpg\n",
      "Saved visualization to C:/Users/X039784/projects/pytorch_objectdetection\\visualized_results\\detected_images-14-_jpg.rf.65d2e5362d768e6d61473b6a8dc5fe0f.jpg\n",
      "Saved visualization to C:/Users/X039784/projects/pytorch_objectdetection\\visualized_results\\detected_images-6-_jpg.rf.4c120fe318928b014756341a9bcd967b.jpg\n",
      "Saved visualization to C:/Users/X039784/projects/pytorch_objectdetection\\visualized_results\\detected_images-8-_jpg.rf.6dcf5795c1ede532513234f97ff42b0a.jpg\n",
      "Saved visualization to C:/Users/X039784/projects/pytorch_objectdetection\\visualized_results\\detected_images.jpg\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Visualization Function\n",
    "def visualize_detections(image, boxes, labels, scores, class_labels, output_path, image_name):\n",
    "    \"\"\"\n",
    "    Visualize detections and save the result as an image.\n",
    "    \"\"\"\n",
    "    # Move image to CPU and convert to NumPy\n",
    "    image = image.cpu().permute(1, 2, 0).numpy()\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "    ax.imshow(image)  # Display the image\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        box = boxes[i].tolist()\n", "        label = class_labels[labels[i].item()]\n",
    "        score = scores[i].item()\n",
    "\n",
    "        # Draw the bounding box\n",
    "        rect = patches.Rectangle(\n",
    "            (box[0], box[1]), box[2] - box[0], box[3] - box[1],\n",
    "            linewidth=2, edgecolor='r', facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # Add label and score\n",
    "        ax.text(box[0], box[1] - 5, f\"{label}: {score:.2f}\", color='white', backgroundcolor='red')\n",
    "\n",
    "    # Save the visualized image\n",
    "    save_path = os.path.join(output_path, f\"detected_{image_name}\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.close(fig)  # Close the figure to free memory\n",
    "    print(f\"Saved visualization to {save_path}\")\n",
    "\n",
    "# Set up output directory for visualized results\n",
    "OUTPUT_DIR = os.path.join(ROOT_DIR, \"visualized_results\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Run inference and save visualized detections\n",
    "with torch.no_grad():\n",
    "    for images, image_names in inference_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        outputs = model(images)  # Perform inference\n",
    "\n",
    "        for output, image_name, image in zip(outputs, image_names, images):\n",
    "            # Extract outputs\n",
    "            boxes = output['boxes'].detach().cpu()\n",
    "            scores = output['scores'].detach().cpu()\n",
    "            labels = output['labels'].detach().cpu()\n",
    "\n",
    "            # Apply confidence threshold\n",
    "            confidence_threshold = 0.5\n",
    "            valid_indices = scores > confidence_threshold\n",
    "            boxes = boxes[valid_indices]\n",
    "            scores = scores[valid_indices]\n",
    "            labels = labels[valid_indices]\n",
    "\n",
    "            if len(boxes) == 0:\n",
    "                print(f\"No detections for {image_name}\")\n",
    "                continue\n",
    "\n",
    "            # Visualize and save results\n",
    "            visualize_detections(image, boxes, labels, scores, class_labels, OUTPUT_DIR, image_name)\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
